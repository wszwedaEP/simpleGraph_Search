----TO DO----

* answer the question:
  Do multiprocessing interprets the script every time for one core?
  Why in multiprocessing_wrapper.py, print located at the beginning of the script is executed num_cores+1 times?
  Can I understand it, that every core needs to read his own definition of logic (what happens before if __main__)?
  Instead of parsing once and then distributing it for every core

 * mocking (proper calling of 'ejectCD' function, instead of actually ejectingCD)

















 ---DONE---

 Unit testy zalozone na wersje recursive

Multiprocessing i backtrack jako queue. (+koniecznie timeit)
Wydaje mi sie, ze jedyna opcja logicznego dzialania multiprocessingu jest gdy kazdy proces zacznie
przeszukiwac graf od jednego z X punktow poczatkowych i wyczerpuje swoj backtrack;
tzn proces 1 leci a->b i dalej down the rabbit hole
(taki pojedynczy blok kodu obslugujacego wszystkie sciezki wychodzace z a->b nazwiemy ROUTINE)
proces 2 leci a->e,
proces 3 leci a->c itd.
(w ogolnej wersji, procesy musialaby co jakis czas komunikowac sie ze soba i wymienic sie informacja
co kazdy z nich ustalil)
(mamy do dyspozycji pool.map;
 lock,
 apply_async,
 from multiprocess import Process,
 byc moze bedziesz musial korzystac z wiecej niz jednego backtracka
 )



 The join() method, when used with threading or multiprocessing, is not related to str.join()
 - it's not actually concatenating anything together. Rather, it just means "wait for this [thread/process] to complete".
 The name join is used because the multiprocessing module's API is meant to look as similar to the threading module's API

 POOL_STARMAP


* ZALOZ ODDZIELNY PROCES NA KAZDY Z ITERABLE Z BACKTRACK INIT. zrob to przy uzyciu klasy Pool,
    options = self.backtrack <<init_options>>
    pool = Pool(processes=len(num_procs)
    print(pool.map(GraphSolver(tplg, start_point, end_point).solve(), options))


 *  remember about close()